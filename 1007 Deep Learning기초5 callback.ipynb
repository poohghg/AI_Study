{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 딥러닝의 이슈\n",
    "- 데이터가 적고\n",
    "- 모델이 복잡하면 복잡해질수록 overfitting 문제가 발생한다.\n",
    "- -> 정규화 방법을 통해 overfitting을 어는정도 잡을수 있다.\n",
    "- -> dropout 방법 일정 데이터를 랜덤적으로 삭제한다.\n",
    "- -> 앙상블 기법\n",
    "    - 베깅은 전체 데이터의 평균을 내서 합친다.\n",
    "    - 부스팅\n",
    "- -> early stopping\n",
    "    - 학습이 어는정도 돼버리면 더 이상 학습을 시키지 않는다.\n",
    "    \n",
    "- callback을 통해 epoch이 돌때마다 결과값을 확인.\n",
    "- 생성된 모델은 hdf5파일로 저장가능하다.\n",
    "- hdf5파일은 공용파일이다. -> 다양한 언어에서 불러와 모델을 사용할 수 있다."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### https://www.tensorflow.org/tutorials/keras/save_and_load?hl=ko"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [],
   "source": [
    "# epch를 돌고나서 상태확인\n",
    "from tensorflow.keras.callbacks import Callback"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2.0.0\n"
     ]
    }
   ],
   "source": [
    "from __future__ import absolute_import, division, print_function, unicode_literals\n",
    "\n",
    "import os\n",
    "\n",
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "\n",
    "print(tf.version.VERSION)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\nX - 0 / 255 - 0\\n\\n'"
      ]
     },
     "execution_count": 86,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "(train_images, train_labels), (test_images, test_labels) = tf.keras.datasets.mnist.load_data()\n",
    "\n",
    "train_labels = train_labels[:1000]\n",
    "test_labels = test_labels[:1000]\n",
    "# normarllrization -> 정규화\n",
    "# min-max\n",
    "\"\"\"\n",
    "X - 0 / 255 - 0\n",
    "\n",
    "\"\"\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {},
   "outputs": [],
   "source": [
    "# standardization\n",
    "# 천장만 학습\n",
    "train_images = train_images[:1000].reshape(-1, 28 * 28) /255.0\n",
    "test_images = test_images[:1000].reshape(-1, 28 * 28) / 255.0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1000, 784)"
      ]
     },
     "execution_count": 76,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_images.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential_3\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "input (Dense)                (None, 512)               401920    \n",
      "_________________________________________________________________\n",
      "dropout_3 (Dropout)          (None, 512)               0         \n",
      "_________________________________________________________________\n",
      "dense_3 (Dense)              (None, 10)                5130      \n",
      "=================================================================\n",
      "Total params: 407,050\n",
      "Trainable params: 407,050\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "# Define a simple sequential model\n",
    "# 케라스에서 input, demension을 통해 크기를 맞춰준다.\n",
    "# Sequential 방법대신 model 을 생성하여 만들수 있다.\n",
    "def create_model():\n",
    "  model = tf.keras.models.Sequential([\n",
    "    keras.layers.Dense(512, name=\"input\" ,activation='relu', input_shape=(784,)),\n",
    "    # overfitting 을 막기위해 사용\n",
    "    keras.layers.Dropout(0.2),\n",
    "    # multi - classfication\n",
    "    keras.layers.Dense(10, activation='softmax')\n",
    "  ])\n",
    "\n",
    "  model.compile(optimizer='adam',\n",
    "                # sparse는 one - hot encdig 대신 쓰인다.\n",
    "                loss='sparse_categorical_crossentropy',\n",
    "                metrics=['accuracy'])\n",
    "\n",
    "  return model\n",
    "\n",
    "# Create a basic model instance\n",
    "model = create_model()\n",
    "\n",
    "# Display the model's architecture\n",
    "model.summary()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 1000 samples, validate on 1000 samples\n",
      "Epoch 1/3\n",
      " 864/1000 [========================>.....] - ETA: 0s - loss: 1.2155 - accuracy: 0.6516\n",
      "Epoch 00001: saving model to training_1/cp.ckpt1\n",
      "1000/1000 [==============================] - 1s 898us/sample - loss: 1.1316 - accuracy: 0.6770 - val_loss: 0.6975 - val_accuracy: 0.7940\n",
      "Epoch 2/3\n",
      " 928/1000 [==========================>...] - ETA: 0s - loss: 0.4305 - accuracy: 0.8761\n",
      "Epoch 00002: saving model to training_1/cp.ckpt1\n",
      "1000/1000 [==============================] - 0s 460us/sample - loss: 0.4304 - accuracy: 0.8760 - val_loss: 0.5377 - val_accuracy: 0.8230\n",
      "Epoch 3/3\n",
      " 896/1000 [=========================>....] - ETA: 0s - loss: 0.2785 - accuracy: 0.9174\n",
      "Epoch 00003: saving model to training_1/cp.ckpt1\n",
      "1000/1000 [==============================] - 0s 408us/sample - loss: 0.2903 - accuracy: 0.9150 - val_loss: 0.5036 - val_accuracy: 0.8430\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<tensorflow.python.keras.callbacks.History at 0x1b5bd088>"
      ]
     },
     "execution_count": 89,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "checkpoint_path = \"training_1/cp.ckpt1\"\n",
    "checkpoint_dir = os.path.dirname(checkpoint_path)\n",
    "\n",
    "# Create a callback that saves the model's weights\n",
    "cp_callback = tf.keras.callbacks.ModelCheckpoint(filepath=checkpoint_path,\n",
    "                                                 # weight만 저장할것인가\n",
    "                                                 save_weights_only=True,\n",
    "                                                 # 주석\n",
    "                                                 verbose=1)\n",
    "\n",
    "# keras는 s가 붙으면 리스트를 사용한다.\n",
    "\n",
    "model.fit(train_images, \n",
    "          train_labels,  \n",
    "          epochs=3,\n",
    "          validation_data=(test_images,test_labels),\n",
    "          callbacks=[cp_callback])  # Pass callback to training\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1000/1 - 0s - loss: 2.4008 - accuracy: 0.0930\n",
      "훈련되지 않은 모델의 정확도:  9.30%\n"
     ]
    }
   ],
   "source": [
    "model = create_model()\n",
    "\n",
    "loss, acc = model.evaluate(test_images,  test_labels, verbose=2)\n",
    "print(\"훈련되지 않은 모델의 정확도: {:5.2f}%\".format(100*acc))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1000/1 - 0s - loss: 0.4257 - accuracy: 0.8730\n",
      "복원된 모델의 정확도: 87.30%\n"
     ]
    }
   ],
   "source": [
    "model.load_weights(checkpoint_path)\n",
    "loss,acc = model.evaluate(test_images,  test_labels, verbose=2)\n",
    "print(\"복원된 모델의 정확도: {:5.2f}%\".format(100*acc))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING: Logging before flag parsing goes to stderr.\n",
      "W1008 06:34:56.458989 19284 callbacks.py:863] `period` argument is deprecated. Please use `save_freq` to specify the frequency in number of samples seen.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 00005: saving model to training_2/cp-0005.ckpt\n",
      "\n",
      "Epoch 00010: saving model to training_2/cp-0010.ckpt\n",
      "\n",
      "Epoch 00015: saving model to training_2/cp-0015.ckpt\n",
      "\n",
      "Epoch 00020: saving model to training_2/cp-0020.ckpt\n",
      "\n",
      "Epoch 00025: saving model to training_2/cp-0025.ckpt\n",
      "\n",
      "Epoch 00030: saving model to training_2/cp-0030.ckpt\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<tensorflow.python.keras.callbacks.History at 0x2c354548>"
      ]
     },
     "execution_count": 94,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 파일 이름에 에포크 번호를 포함시킵니다(`str.format` 포맷)\n",
    "checkpoint_path = \"training_2/cp-{epoch:04d}.ckpt\"\n",
    "checkpoint_dir = os.path.dirname(checkpoint_path)\n",
    "\n",
    "cp_callback = tf.keras.callbacks.ModelCheckpoint(\n",
    "    checkpoint_path, verbose=1, save_weights_only=True,\n",
    "    # 다섯 번째 에포크마다 가중치를 저장합니다\n",
    "    period=5)\n",
    "\n",
    "model = create_model()\n",
    "model.save_weights(checkpoint_path.format(epoch=0))\n",
    "model.fit(train_images, train_labels,\n",
    "          epochs = 30, callbacks = [cp_callback],\n",
    "          validation_data = (test_images,test_labels),\n",
    "          verbose=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'training_2\\\\cp-0030.ckpt'"
      ]
     },
     "execution_count": 98,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#노트: 텐서플로는 기본적으로 최근 5개의 체크포인트만 저장합니다.\n",
    "latest = tf.train.latest_checkpoint(checkpoint_dir)\n",
    "latest"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1000/1 - 0s - loss: 0.4809 - accuracy: 0.8760\n",
      "복원된 모델의 정확도: 87.60%\n"
     ]
    }
   ],
   "source": [
    "model = create_model()\n",
    "model.load_weights(latest)\n",
    "loss, acc = model.evaluate(test_images,  test_labels, verbose=2)\n",
    "print(\"복원된 모델의 정확도: {:5.2f}%\".format(100*acc))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1000/1 - 0s - loss: 0.4809 - accuracy: 0.8760\n",
      "복원된 모델의 정확도: 87.60%\n"
     ]
    }
   ],
   "source": [
    "# 가중치를 저장합니다\n",
    "model.save_weights('./checkpoints/my_checkpoint')\n",
    "\n",
    "# 가중치를 복원합니다\n",
    "model = create_model()\n",
    "model.load_weights('./checkpoints/my_checkpoint')\n",
    "\n",
    "loss,acc = model.evaluate(test_images,  test_labels, verbose=2)\n",
    "print(\"복원된 모델의 정확도: {:5.2f}%\".format(100*acc))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "metadata": {},
   "outputs": [],
   "source": [
    "## 전체 모델을 HDF5 파일로 저장합니다\n",
    "model.save('my_model.h5')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "W1008 06:43:09.173171 19284 util.py:144] Unresolved object in checkpoint: (root).optimizer.iter\n",
      "W1008 06:43:09.174171 19284 util.py:144] Unresolved object in checkpoint: (root).optimizer.beta_1\n",
      "W1008 06:43:09.176171 19284 util.py:144] Unresolved object in checkpoint: (root).optimizer.beta_2\n",
      "W1008 06:43:09.177171 19284 util.py:144] Unresolved object in checkpoint: (root).optimizer.decay\n",
      "W1008 06:43:09.178171 19284 util.py:144] Unresolved object in checkpoint: (root).optimizer.learning_rate\n",
      "W1008 06:43:09.180171 19284 util.py:152] A checkpoint was restored (e.g. tf.train.Checkpoint.restore or tf.keras.Model.load_weights) but not all checkpointed values were used. See above for specific issues. Use expect_partial() on the load status object, e.g. tf.train.Checkpoint.restore(...).expect_partial(), to silence these warnings, or use assert_consumed() to make the check explicit. See https://www.tensorflow.org/alpha/guide/checkpoints#loading_mechanics for details.\n",
      "W1008 06:43:09.185171 19284 util.py:144] Unresolved object in checkpoint: (root).optimizer.iter\n",
      "W1008 06:43:09.189172 19284 util.py:144] Unresolved object in checkpoint: (root).optimizer.beta_1\n",
      "W1008 06:43:09.190172 19284 util.py:144] Unresolved object in checkpoint: (root).optimizer.beta_2\n",
      "W1008 06:43:09.192172 19284 util.py:144] Unresolved object in checkpoint: (root).optimizer.decay\n",
      "W1008 06:43:09.194172 19284 util.py:144] Unresolved object in checkpoint: (root).optimizer.learning_rate\n",
      "W1008 06:43:09.196172 19284 util.py:152] A checkpoint was restored (e.g. tf.train.Checkpoint.restore or tf.keras.Model.load_weights) but not all checkpointed values were used. See above for specific issues. Use expect_partial() on the load status object, e.g. tf.train.Checkpoint.restore(...).expect_partial(), to silence these warnings, or use assert_consumed() to make the check explicit. See https://www.tensorflow.org/alpha/guide/checkpoints#loading_mechanics for details.\n",
      "W1008 06:43:09.201172 19284 util.py:144] Unresolved object in checkpoint: (root).optimizer.iter\n",
      "W1008 06:43:09.205173 19284 util.py:144] Unresolved object in checkpoint: (root).optimizer.beta_1\n",
      "W1008 06:43:09.206172 19284 util.py:144] Unresolved object in checkpoint: (root).optimizer.beta_2\n",
      "W1008 06:43:09.208173 19284 util.py:144] Unresolved object in checkpoint: (root).optimizer.decay\n",
      "W1008 06:43:09.209173 19284 util.py:144] Unresolved object in checkpoint: (root).optimizer.learning_rate\n",
      "W1008 06:43:09.210173 19284 util.py:152] A checkpoint was restored (e.g. tf.train.Checkpoint.restore or tf.keras.Model.load_weights) but not all checkpointed values were used. See above for specific issues. Use expect_partial() on the load status object, e.g. tf.train.Checkpoint.restore(...).expect_partial(), to silence these warnings, or use assert_consumed() to make the check explicit. See https://www.tensorflow.org/alpha/guide/checkpoints#loading_mechanics for details.\n",
      "W1008 06:43:09.216173 19284 util.py:144] Unresolved object in checkpoint: (root).optimizer.iter\n",
      "W1008 06:43:09.217173 19284 util.py:144] Unresolved object in checkpoint: (root).optimizer.beta_1\n",
      "W1008 06:43:09.218173 19284 util.py:144] Unresolved object in checkpoint: (root).optimizer.beta_2\n",
      "W1008 06:43:09.219173 19284 util.py:144] Unresolved object in checkpoint: (root).optimizer.decay\n",
      "W1008 06:43:09.219173 19284 util.py:144] Unresolved object in checkpoint: (root).optimizer.learning_rate\n",
      "W1008 06:43:09.221174 19284 util.py:152] A checkpoint was restored (e.g. tf.train.Checkpoint.restore or tf.keras.Model.load_weights) but not all checkpointed values were used. See above for specific issues. Use expect_partial() on the load status object, e.g. tf.train.Checkpoint.restore(...).expect_partial(), to silence these warnings, or use assert_consumed() to make the check explicit. See https://www.tensorflow.org/alpha/guide/checkpoints#loading_mechanics for details.\n",
      "W1008 06:43:09.226174 19284 util.py:144] Unresolved object in checkpoint: (root).optimizer.iter\n",
      "W1008 06:43:09.227174 19284 util.py:144] Unresolved object in checkpoint: (root).optimizer.beta_1\n",
      "W1008 06:43:09.229174 19284 util.py:144] Unresolved object in checkpoint: (root).optimizer.beta_2\n",
      "W1008 06:43:09.232174 19284 util.py:144] Unresolved object in checkpoint: (root).optimizer.decay\n",
      "W1008 06:43:09.233174 19284 util.py:144] Unresolved object in checkpoint: (root).optimizer.learning_rate\n",
      "W1008 06:43:09.234174 19284 util.py:152] A checkpoint was restored (e.g. tf.train.Checkpoint.restore or tf.keras.Model.load_weights) but not all checkpointed values were used. See above for specific issues. Use expect_partial() on the load status object, e.g. tf.train.Checkpoint.restore(...).expect_partial(), to silence these warnings, or use assert_consumed() to make the check explicit. See https://www.tensorflow.org/alpha/guide/checkpoints#loading_mechanics for details.\n",
      "W1008 06:43:09.498189 19284 hdf5_format.py:198] Error in loading the saved optimizer state. As a result, your model is starting with a freshly initialized optimizer.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential_11\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "input (Dense)                (None, 512)               401920    \n",
      "_________________________________________________________________\n",
      "dropout_11 (Dropout)         (None, 512)               0         \n",
      "_________________________________________________________________\n",
      "dense_11 (Dense)             (None, 10)                5130      \n",
      "=================================================================\n",
      "Total params: 407,050\n",
      "Trainable params: 407,050\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "# 가중치와 옵티마이저를 포함하여 정확히 동일한 모델을 다시 생성합니다\n",
    "new_model = keras.models.load_model('my_model.h5')\n",
    "new_model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1000/1 - 0s - loss: 0.4809 - accuracy: 0.8760\n",
      "복원된 모델의 정확도: 87.60%\n"
     ]
    }
   ],
   "source": [
    "loss, acc = new_model.evaluate(test_images,  test_labels, verbose=2)\n",
    "print(\"복원된 모델의 정확도: {:5.2f}%\".format(100*acc))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
