{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 딥러닝의 이슈\n",
    "- 데이터가 적고\n",
    "- 모델이 복잡하면 복잡해질수록 overfitting 문제가 발생한다.\n",
    "- -> 정규화 방법을 통해 overfitting을 어는정도 잡을수 있다.\n",
    "- -> dropout 방법 일정 데이터를 랜덤적으로 삭제한다.\n",
    "- -> 앙상블 기법\n",
    "    - 베깅은 전체 데이터의 평균을 내서 합친다.\n",
    "    - 부스팅\n",
    "- -> early stopping\n",
    "    - 학습이 어는정도 돼버리면 더 이상 학습을 시키지 않는다.\n",
    "    \n",
    "- callback을 통해 epoch이 돌때마다 결과값을 확인.\n",
    "- 생성된 모델은 hdf5파일로 저장가능하다.\n",
    "- hdf5파일은 공용파일이다. -> 다양한 언어에서 불러와 모델을 사용할 수 있다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# epch를 돌고나서 하는것.\n",
    "from tensorflow.keras.callbacks import Callback"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2.0.0\n"
     ]
    }
   ],
   "source": [
    "from __future__ import absolute_import, division, print_function, unicode_literals\n",
    "\n",
    "import os\n",
    "\n",
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "\n",
    "print(tf.version.VERSION)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "(train_images, train_labels), (test_images, test_labels) = tf.keras.datasets.mnist.load_data()\n",
    "\n",
    "train_labels = train_labels[:1000]\n",
    "test_labels = test_labels[:1000]\n",
    "# normarllrization -> 정규화\n",
    "# min-max\n",
    "\"\"\"\n",
    "X - 0 / 255 - 0\n",
    "\n",
    "\"\"\"\n",
    "# standardization\n",
    "train_images = train_images[:1000].reshape(-1, 28 * 28) / 255.0\n",
    "test_images = test_images[:1000].reshape(-1, 28 * 28) / 255.0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential_4\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "sun (Dense)                  (None, 512)               401920    \n",
      "_________________________________________________________________\n",
      "dropout_4 (Dropout)          (None, 512)               0         \n",
      "_________________________________________________________________\n",
      "dense_6 (Dense)              (None, 10)                5130      \n",
      "=================================================================\n",
      "Total params: 407,050\n",
      "Trainable params: 407,050\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "# Define a simple sequential model\n",
    "# 케라스에서 input, demension을 통해 크기를 맞춰준다.\n",
    "# Sequential 방법대신 model 을 생성하여 만들수 있다.\n",
    "def create_model():\n",
    "  model = tf.keras.models.Sequential([\n",
    "    keras.layers.Dense(512, name=\"sun\" ,activation='relu', input_shape=(784,)),\n",
    "    # overfitting 을 막기위해 사용\n",
    "    keras.layers.Dropout(0.2),\n",
    "    # multi - classfication\n",
    "    keras.layers.Dense(10, activation='softmax')\n",
    "  ])\n",
    "\n",
    "  model.compile(optimizer='adam',\n",
    "                # sparse는 one - hot encdig 대신 쓰인다.\n",
    "                loss='sparse_categorical_crossentropy',\n",
    "                metrics=['accuracy'])\n",
    "\n",
    "  return model\n",
    "\n",
    "# Create a basic model instance\n",
    "model = create_model()\n",
    "\n",
    "# Display the model's architecture\n",
    "model.summary()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 1000 samples, validate on 1000 samples\n",
      "Epoch 1/10\n",
      " 672/1000 [===================>..........] - ETA: 0s - loss: 1.3856 - accuracy: 0.5774\n",
      "Epoch 00001: saving model to training_1/cp.ckpt1\n",
      "1000/1000 [==============================] - 0s 464us/sample - loss: 1.1606 - accuracy: 0.6420 - val_loss: 0.6991 - val_accuracy: 0.7780\n",
      "Epoch 2/10\n",
      " 576/1000 [================>.............] - ETA: 0s - loss: 0.4526 - accuracy: 0.8663\n",
      "Epoch 00002: saving model to training_1/cp.ckpt1\n",
      "1000/1000 [==============================] - 0s 164us/sample - loss: 0.4304 - accuracy: 0.8780 - val_loss: 0.5431 - val_accuracy: 0.8300\n",
      "Epoch 3/10\n",
      " 576/1000 [================>.............] - ETA: 0s - loss: 0.2942 - accuracy: 0.9271\n",
      "Epoch 00003: saving model to training_1/cp.ckpt1\n",
      "1000/1000 [==============================] - 0s 180us/sample - loss: 0.2995 - accuracy: 0.9260 - val_loss: 0.4804 - val_accuracy: 0.8490\n",
      "Epoch 4/10\n",
      " 640/1000 [==================>...........] - ETA: 0s - loss: 0.2164 - accuracy: 0.9469\n",
      "Epoch 00004: saving model to training_1/cp.ckpt1\n",
      "1000/1000 [==============================] - 0s 164us/sample - loss: 0.2071 - accuracy: 0.9490 - val_loss: 0.4320 - val_accuracy: 0.8560\n",
      "Epoch 5/10\n",
      " 640/1000 [==================>...........] - ETA: 0s - loss: 0.1503 - accuracy: 0.9641\n",
      "Epoch 00005: saving model to training_1/cp.ckpt1\n",
      "1000/1000 [==============================] - 0s 181us/sample - loss: 0.1587 - accuracy: 0.9610 - val_loss: 0.4499 - val_accuracy: 0.8580\n",
      "Epoch 6/10\n",
      " 576/1000 [================>.............] - ETA: 0s - loss: 0.1123 - accuracy: 0.9861\n",
      "Epoch 00006: saving model to training_1/cp.ckpt1\n",
      "1000/1000 [==============================] - 0s 176us/sample - loss: 0.1171 - accuracy: 0.9770 - val_loss: 0.4107 - val_accuracy: 0.8670\n",
      "Epoch 7/10\n",
      " 512/1000 [==============>...............] - ETA: 0s - loss: 0.0871 - accuracy: 0.9902\n",
      "Epoch 00007: saving model to training_1/cp.ckpt1\n",
      "1000/1000 [==============================] - 0s 186us/sample - loss: 0.0875 - accuracy: 0.9870 - val_loss: 0.3944 - val_accuracy: 0.8760\n",
      "Epoch 8/10\n",
      " 576/1000 [================>.............] - ETA: 0s - loss: 0.0671 - accuracy: 0.9913\n",
      "Epoch 00008: saving model to training_1/cp.ckpt1\n",
      "1000/1000 [==============================] - 0s 160us/sample - loss: 0.0688 - accuracy: 0.9900 - val_loss: 0.4287 - val_accuracy: 0.8610\n",
      "Epoch 9/10\n",
      " 640/1000 [==================>...........] - ETA: 0s - loss: 0.0436 - accuracy: 1.0000\n",
      "Epoch 00009: saving model to training_1/cp.ckpt1\n",
      "1000/1000 [==============================] - 0s 159us/sample - loss: 0.0493 - accuracy: 0.9970 - val_loss: 0.4019 - val_accuracy: 0.8740\n",
      "Epoch 10/10\n",
      " 672/1000 [===================>..........] - ETA: 0s - loss: 0.0347 - accuracy: 1.0000\n",
      "Epoch 00010: saving model to training_1/cp.ckpt1\n",
      "1000/1000 [==============================] - 0s 174us/sample - loss: 0.0377 - accuracy: 1.0000 - val_loss: 0.4008 - val_accuracy: 0.8740\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<tensorflow.python.keras.callbacks.History at 0x196b02805c0>"
      ]
     },
     "execution_count": 56,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "checkpoint_path = \"training_1/cp.ckpt1\"\n",
    "checkpoint_dir = os.path.dirname(checkpoint_path)\n",
    "\n",
    "# Create a callback that saves the model's weights\n",
    "cp_callback = tf.keras.callbacks.ModelCheckpoint(filepath=checkpoint_path,\n",
    "                                                 # weight만 저장할것인가\n",
    "                                                 save_weights_only=True,\n",
    "                                                 # 주석\n",
    "                                                 verbose=1)\n",
    "\n",
    "# keras는 s가 붙으면 리스트를 사용한다.\n",
    "\n",
    "model.fit(train_images, \n",
    "          train_labels,  \n",
    "          epochs=10,\n",
    "          validation_data=(test_images,test_labels),\n",
    "          callbacks=[cp_callback])  # Pass callback to training\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.save()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
